{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "1.What is Simple Linear Regression?\n",
        "\n",
        "Simple linear regression aims to find a linear relationship to describe the correlation between an independent and possibly dependent variable. The regression line can be used to predict or estimate missing values, this is known as interpolation.\n"
      ],
      "metadata": {
        "id": "B0A4kfe6Zk0_"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "2.What are the key assumptions of Simple Linear Regression\n",
        "\n",
        "The key assumptions of simple linear regression are linearity, independence, homoscedasticity, and normality of residuals.\n",
        "1. Linearity: The relationship between the independent variable (X) and the dependent variable (Y) is linear, meaning they can be represented by a straight line.\n",
        "2. Independence: The errors or residuals (the difference between the actual Y value and the predicted Y value) are independent of each other.\n",
        "3. Homoscedasticity: The variance of the errors is constant across all levels of the independent variable (X). This means the spread of the errors is consistent, not widening or narrowing as X changes.\n",
        "4. Normality of Residuals: The errors (residuals) are normally distributed."
      ],
      "metadata": {
        "id": "5xeQzy_WZx_l"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "3.What does the coefficient m represent in the equation Y=mX+c\n",
        "\n",
        "In the equation y = mx + c, the coefficient 'm' represents the slope or gradient of the line. It indicates how steeply the line rises or falls as x increases."
      ],
      "metadata": {
        "id": "U67a8CB3Z8Gq"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "4.What does the intercept c represent in the equation Y=mX+c\n",
        "\n",
        "In the equation y = mx + c, the \"c\" represents the y-intercept. This means it's the y-coordinate of the point where the line crosses the y-axis on a graph."
      ],
      "metadata": {
        "id": "R7wIgYkSaFHG"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "5.How do we calculate the slope m in Simple Linear Regression\n",
        "\n",
        "The slope (m) in simple linear regression is calculated using the formula: m = (n * Σxy - Σx * Σy) / (n * Σx² - (Σx)²). This formula utilizes the sums of the independent (x) and dependent (y) variable values, their products (xy), and the squares of the independent variable values (x²)."
      ],
      "metadata": {
        "id": "dL76nOn_aKm6"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "6.What is the purpose of the least squares method in Simple Linear Regression\n",
        "\n",
        "The least squares method is a mathematical technique that allows the analyst to determine the best way of fitting a curve on top of a chart of data points. It is widely used to make scatter plots easier to interpret and is associated with regression analysis.\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "xpw73FpEaUln"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "7. How is the coefficient of determination (R²) interpreted in Simple Linear Regression\n",
        "\n",
        "In simple linear regression, the coefficient of determination (R²) represents the proportion of variance in the dependent variable that is explained by the independent variable. It essentially tells you how well the regression line fits the data points. A higher R² indicates a better fit, meaning more of the variability in the dependent variable."
      ],
      "metadata": {
        "id": "wtyPcRYdaf3u"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "8. What is Multiple Linear Regression\n",
        "\n",
        "Multiple linear regression is a statistical technique used to model the relationship between a dependent variable and two or more independent variables, where the relationship is assumed to be linear. It's an extension of simple linear regression, which only uses one independent variable. The goal is to predict the value of the dependent variable based on the values of the independent variables."
      ],
      "metadata": {
        "id": "eAJ15LP8avet"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "9.- What is the main difference between Simple and Multiple Linear Regression\n",
        "\n",
        "Simple linear regression uses one independent variable to predict a dependent variable, while multiple linear regression uses two or more independent variables to predict the same dependent variable. Simple linear regression models a linear relationship between a single predictor and a response variable, while multiple linear regression models a linear relationship between multiple predictors and a response variable."
      ],
      "metadata": {
        "id": "pHe10PIuawNF"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "10.What are the key assumptions of Multiple Linear Regression\n",
        "\n",
        "The key assumptions of multiple linear regression include a linear relationship between variables, no multicollinearity among independent variables, homoscedasticity (constant variance of errors), and normally distributed residuals. Additionally, observations should be independent of each other, and there should be no influential outliers."
      ],
      "metadata": {
        "id": "7S5_RoznbG96"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "11.What is heteroscedasticity, and how does it affect the results of a Multiple Linear Regression model\n",
        "\n",
        "Heteroscedasticity in a Multiple Linear Regression model refers to a situation where the variance of the error term (residuals) is not constant across all values of the independent variables. In other words, the spread of the data points around the regression line is not consistent. This violates one of the assumptions of Ordinary Least Squares (OLS) regression, which assumes homoscedasticity (constant variance)."
      ],
      "metadata": {
        "id": "76Y_q280bNvv"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "12.How can you improve a Multiple Linear Regression model with high multicollinearity\n",
        "\n",
        "To improve a multiple linear regression model with high multicollinearity, several strategies can be employed. These include removing or combining highly correlated variables, using regularization techniques like Ridge or Lasso regression, collecting more data, or employing dimensionality reduction methods like Principal Component Analysis (PCA)."
      ],
      "metadata": {
        "id": "gBtCY1u9baTE"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "13.What are some common techniques for transforming categorical variables for use in regression models\n",
        "\n",
        "\n",
        "Several techniques transform categorical variables for use in regression models. One-hot encoding is a common approach, creating binary variables for each category. Label encoding assigns unique numerical values, which can be helpful but may imply an ordinal relationship if one doesn't exist. Ordinal encoding allows explicit mapping of categories to integers when an order is defined."
      ],
      "metadata": {
        "id": "7DcCts0TbhfH"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "14.What is the role of interaction terms in Multiple Linear Regression\n",
        "\n",
        "In multiple linear regression, we can use an interaction term when the relationship between two variables is moderated by a third variable. This allows the slope coefficient for one variable to vary depending on the value of the other variable."
      ],
      "metadata": {
        "id": "zRGxz5O8brdl"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "15.How can the interpretation of intercept differ between Simple and Multiple Linear Regression\n",
        "\n",
        "In both simple and multiple linear regression, the intercept represents the predicted value of the dependent variable when all independent variables are zero. However, in multiple regression, the intercept's interpretation becomes nuanced because it's the predicted value when all predictors are zero, not just the single predictor in simple regression.\n"
      ],
      "metadata": {
        "id": "GSwBT6dDb8TE"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "16.What is the significance of the slope in regression analysis, and how does it affect predictions\n",
        "\n",
        "In regression analysis, the slope represents the rate of change in the dependent variable (y) for each unit change in the independent variable (x). A steeper slope indicates a greater change in y for the same change in x, while a flatter slope suggests a smaller change. The slope's sign (positive or negative) indicates the direction of the relationship: positive slope means y increases as x increases, and negative slope means y decreases as x increases."
      ],
      "metadata": {
        "id": "tbR1fSQecD-J"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "17.How does the intercept in a regression model provide context for the relationship between variables\n",
        "\n",
        "The intercept in a regression model provides the predicted value of the dependent variable when all independent variables are zero. This value serves as a baseline or starting point, helping to understand the relationship between variables by showing what the outcome variable is expected to be when the independent variables are not influencing it."
      ],
      "metadata": {
        "id": "eGXFQXkOcZgm"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "18.What are the limitations of using R² as a sole measure of model performance\n",
        "\n",
        "R² is a useful metric for evaluating regression models, but it has limitations when used alone. It doesn't capture the full picture of model performance and can be misleading if not considered in conjunction with other metrics. For example, R² can be high even with a poor model, or low even with a good model.\n",
        "Here's a more detailed look at the limitations:\n",
        "1. Doesn't indicate model reliability or goodness of fit: R² only measures the proportion of variance explained by the model, not the reliability or accuracy of the model's predictions. A high R² doesn't guarantee a good model, and a low R² doesn't necessarily mean the model is poor.\n",
        "2. Sensitive to adding more predictors: R² tends to increase with the addition of more predictor variables, even if those variables aren't truly useful for prediction. This can lead to overfitting, where the model performs well on the training data but poorly on new data"
      ],
      "metadata": {
        "id": "kFcsRgyCc0dK"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "19.How would you interpret a large standard error for a regression coefficient\n",
        "\n",
        "A large standard error for a regression coefficient suggests that the estimated coefficient is likely to be a less precise estimate of the true population coefficient. In other words, there's more uncertainty about the true value of the coefficient, meaning it's less likely to be close to the true population value. This can indicate several issues with the model, such as a high degree of variability in the data, a poor model fit, or multicollinearity among the independent variables"
      ],
      "metadata": {
        "id": "X-nFyL6PdAgE"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "20.How can heteroscedasticity be identified in residual plots, and why is it important to address it\n",
        "\n",
        "Heteroscedasticity, or unequal variance of errors, in residual plots is indicated by a \"fan\" or \"cone\" shape, where the spread of residuals increases or decreases with the fitted values. Addressing heteroscedasticity is crucial because it can lead to misleading statistical inferences, even if the model estimates remain unbiased."
      ],
      "metadata": {
        "id": "IZICS2RwdOjI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "21.What does it mean if a Multiple Linear Regression model has a high R² but low adjusted R2\n",
        "\n",
        "A high R² (coefficient of determination) and a low adjusted R² in a multiple linear regression model suggest that the model is likely overfitting the data, meaning it's capturing noise or random fluctuations in the data rather than genuine relationships."
      ],
      "metadata": {
        "id": "DbjUHcrMdZOL"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "22.Why is it important to scale variables in Multiple Linear Regression\n",
        "\n",
        "Scaling variables in Multiple Linear Regression is important because it can improve model performance, interpretability, and stability, especially when dealing with variables on different scales or when using algorithms sensitive to feature scaling. Scaling ensures that all features contribute equally to the model's learning process and that the coefficients are more easily comparable, leading to a more reliable and accurate model."
      ],
      "metadata": {
        "id": "AmftyY8mdi41"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "23.What is polynomial regression\n",
        "\n",
        "Polynomial regression is a type of regression analysis that models the relationship between a dependent variable and one or more independent variables using a polynomial equation. Unlike linear regression, which assumes a straight-line relationship, polynomial regression can capture more complex, non-linear patterns by fitting curved lines to the data."
      ],
      "metadata": {
        "id": "yzCCTlaudspE"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "24.How does polynomial regression differ from linear regression\n",
        "\n",
        "Polynomial regression differs from linear regression in that it models the relationship between variables as a polynomial equation rather than a straight line. This allows polynomial regression to capture non-linear relationships in data that linear regression cannot. Linear regression assumes a straight line relationship, while polynomial regression can fit curves to the data."
      ],
      "metadata": {
        "id": "iqSSEso9d3ye"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "25.When is polynomial regression used\n",
        "\n",
        "Polynomial regression is used when the relationship between independent and dependent variables is not linear, but rather exhibits a curvilinear pattern. This occurs when a straight line cannot adequately capture the data's trend, and a curved line is needed for a better fit. In essence, it extends linear regression by allowing for non-linear relationships through the use of polynomial terms."
      ],
      "metadata": {
        "id": "ozpUvJQHeA0S"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "26.What is the general equation for polynomial regression\n",
        "\n",
        "You are using polynomial regression when you predict Y using a single X variable together with some of its powers (X2, X3, etc.). Let us consider just the case of X with X2. With these variables, the usual multiple regression equation, Y = a + b1X1 + b2X2, becomes the quadratic polynomial Y = a + b1X + b2X2."
      ],
      "metadata": {
        "id": "taatsjnQeI8x"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "27.Can polynomial regression be applied to multiple variables\n",
        "\n",
        "Yes, polynomial regression can be applied to multiple variables. It extends linear regression by introducing higher-order terms of the independent variables, allowing for the modeling of more complex, non-linear relationships. This also enables the modeling of interactions between the variables."
      ],
      "metadata": {
        "id": "FFc0g8XLeRMm"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "28.What are the limitations of polynomial regression\n",
        "\n",
        "Polynomial regression, while versatile, has limitations including the risk of overfitting with high-degree polynomials, computational complexity, and the difficulty of selecting the optimal degree. It can also be sensitive to outliers and may not be suitable for modeling certain types of relationships, such as those that involve logarithmic patterns or sudden changes in acceleration"
      ],
      "metadata": {
        "id": "s7z6D0hwea0C"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "29.What methods can be used to evaluate model fit when selecting the degree of a polynomial\n",
        "\n",
        "\n",
        "Several methods can be used to evaluate model fit when selecting the degree of a polynomial, including cross-validation, visual inspection, and statistical measures like R-squared and AIC/BIC. Cross-validation helps prevent overfitting, while visual inspection and statistical measures aid in determining the right balance between simplicity and accuracy."
      ],
      "metadata": {
        "id": "RLZP4GOBeg7u"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "30.Why is visualization important in polynomial regression\u001d\n",
        "\n",
        "Visualization is crucial in polynomial regression for understanding and interpreting the model's fit to the data. It helps to visually assess whether a polynomial model is appropriate and to identify potential problems like overfitting or underfitting. By plotting the data points and the fitted polynomial curve, it becomes easier to see how well the model captures the underlying relationship between variables."
      ],
      "metadata": {
        "id": "LaHqQH4WerMY"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "31.How is polynomial regression implemented in Python?\n",
        "\n",
        "Implement Polynomial Regression in Python\n",
        "Step 1: Import the required python packages. ...\n",
        "Step 2: Load the dataset. ...\n",
        "Step 3: Data analysis. ...\n",
        "Step 4: Split the dataset into dependent/independent variables. ...\n",
        "Step 5: Train the regression model. ...\n",
        "Step 6: Predict the result."
      ],
      "metadata": {
        "id": "FnRdKO0ge1TP"
      }
    }
  ]
}